
@incollection{tranSpectralSignaturesBackdoor2018,
  title = {Spectral {{Signatures}} in {{Backdoor Attacks}}},
  url = {http://papers.nips.cc/paper/8024-spectral-signatures-in-backdoor-attacks.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-22},
  date = {2018},
  pages = {8000--8010},
  author = {Tran, Brandon and Li, Jerry and Madry, Aleksander},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {C:\\Users\\shant\\Zotero\\storage\\UVGUUTSM\\Tran et al. - 2018 - Spectral Signatures in Backdoor Attacks.pdf;C:\\Users\\shant\\Zotero\\storage\\EE3B3TZP\\8024-spectral-signatures-in-backdoor-attacks.html}
}

@ARTICLE{chenDetectingBackdoorAttacks2018,
       author = {{Chen}, Bryant and {Carvalho}, Wilka and {Baracaldo}, Nathalie and
         {Ludwig}, Heiko and {Edwards}, Benjamin and {Lee}, Taesung and
         {Molloy}, Ian and {Srivastava}, Biplav},
        title = "{Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
         year = "2018",
        month = "Nov",
          eid = {arXiv:1811.03728},
        pages = {arXiv:1811.03728},
archivePrefix = {arXiv},
       eprint = {1811.03728},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv181103728C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{turnerCleanLabelBackdoorAttacks2018,
  title = {Clean-{{Label Backdoor Attacks}}},
  url = {https://openreview.net/forum?id=HJg6e2CcK7},
  abstract = {Deep neural networks have been recently demonstrated to be vulnerable to backdoor attacks. Specifically, by altering a small set of training examples, an adversary is able to install a backdoor...},
  urldate = {2019-05-22},
  date = {2018-09-27},
  author = {Turner, Alexander and Tsipras, Dimitris and Madry, Aleksander},
  file = {C:\\Users\\shant\\Zotero\\storage\\LCGHS5VF\\Turner et al. - 2018 - Clean-Label Backdoor Attacks.pdf;C:\\Users\\shant\\Zotero\\storage\\XEX844MT\\forum.html}
}

@ARTICLE{shlensTutorialIndependentComponent2014,
       author = {{Shlens}, Jonathon},
        title = "{A Tutorial on Independent Component Analysis}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2014",
        month = "Apr",
          eid = {arXiv:1404.2986},
        pages = {arXiv:1404.2986},
archivePrefix = {arXiv},
       eprint = {1404.2986},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1404.2986S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{yangGenerativePoisoningAttack2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.01340},
  primaryClass = {cs, stat},
  title = {Generative {{Poisoning Attack Method Against Neural Networks}}},
  url = {http://arxiv.org/abs/1703.01340},
  abstract = {Poisoning attack is identified as a severe security threat to machine learning algorithms. In many applications, for example, deep neural network (DNN) models collect public data as the inputs to perform re-training, where the input data can be poisoned. Although poisoning attack against support vector machines (SVM) has been extensively studied before, there is still very limited knowledge about how such attack can be implemented on neural networks (NN), especially DNNs. In this work, we first examine the possibility of applying traditional gradient-based method (named as the direct gradient method) to generate poisoned data against NNs by leveraging the gradient of the target model w.r.t. the normal data. We then propose a generative method to accelerate the generation rate of the poisoned data: an auto-encoder (generator) used to generate poisoned data is updated by a reward function of the loss, and the target NN model (discriminator) receives the poisoned data to calculate the loss w.r.t. the normal data. Our experiment results show that the generative method can speed up the poisoned data generation rate by up to 239.38x compared with the direct gradient method, with slightly lower model accuracy degradation. A countermeasure is also designed to detect such poisoning attack methods by checking the loss of the target model.},
  urldate = {2019-05-23},
  date = {2017-03-03},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Yang, Chaofei and Wu, Qing and Li, Hai and Chen, Yiran},
  file = {C:\\Users\\shant\\Zotero\\storage\\VQ7SIZKZ\\Yang et al. - 2017 - Generative Poisoning Attack Method Against Neural .pdf;C:\\Users\\shant\\Zotero\\storage\\XB6WU7NP\\1703.html}
}

@inproceedings{wangNeuralCleanseIdentifying,
  title={Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. Available online: \url{http://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf}},
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
  booktitle={IEEE Symposium on Security and Privacy},
  year = {2019}
 }

@ARTICLE{eykholtRobustPhysicalWorldAttacks2017,
       author = {{Eykholt}, Kevin and {Evtimov}, Ivan and {Fernandes}, Earlence and
         {Li}, Bo and {Rahmati}, Amir and {Xiao}, Chaowei and {Prakash}, Atul and
         {Kohno}, Tadayoshi and {Song}, Dawn},
        title = "{Robust Physical-World Attacks on Deep Learning Models}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = "2017",
        month = "Jul",
          eid = {arXiv:1707.08945},
        pages = {arXiv:1707.08945},
archivePrefix = {arXiv},
       eprint = {1707.08945},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170708945E},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{goodfellowExplainingHarnessingAdversarial2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6572},
  primaryClass = {cs, stat},
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  url = {http://arxiv.org/abs/1412.6572},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  urldate = {2019-05-26},
  date = {2014-12-19},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  file = {C:\\Users\\shant\\Zotero\\storage\\V7IL5EYW\\Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf;C:\\Users\\shant\\Zotero\\storage\\3JUAN4QY\\1412.html}
}

@ARTICLE{szegedyIntriguingPropertiesNeural2013,
       author = {{Szegedy}, Christian and {Zaremba}, Wojciech and {Sutskever}, Ilya and
         {Bruna}, Joan and {Erhan}, Dumitru and {Goodfellow}, Ian and
         {Fergus}, Rob},
        title = "{Intriguing properties of neural networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2013",
        month = "Dec",
          eid = {arXiv:1312.6199},
        pages = {arXiv:1312.6199},
archivePrefix = {arXiv},
       eprint = {1312.6199},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2013arXiv1312.6199S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{vorobeychikAdversarialMachineLearning2018,
  title={Adversarial Machine Learning},
  author={Vorobeychik, Y. and Kantarcioglu, M. and Brachman, R.},
  isbn={9781681733951},
  series={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  url={https://books.google.com/books?id=WwXPugEACAAJ},
  year={2018},
  publisher={Morgan \& Claypool Publishers}
}


@article{stallkampManVsComputer2012,
title = "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition",
journal = "Neural Networks",
volume = "32",
pages = "323 - 332",
year = "2012",
note = "Selected Papers from IJCNN 2011",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2012.02.016",
url = "http://www.sciencedirect.com/science/article/pii/S0893608012000457",
author = "J. Stallkamp and M. Schlipsing and J. Salmen and C. Igel",
keywords = "Traffic sign recognition, Machine learning, Convolutional neural networks, Benchmarking",
abstract = "Traffic signs are characterized by a wide variability in their visual appearance in real-world environments. For example, changes of illumination, varying weather conditions and partial occlusions impact the perception of road signs. In practice, a large number of different sign classes needs to be recognized with very high accuracy. Traffic signs have been designed to be easily readable for humans, who perform very well at this task. For computer systems, however, classifying traffic signs still seems to pose a challenging pattern recognition problem. Both image processing and machine learning algorithms are continuously refined to improve on this task. But little systematic comparison of such systems exist. What is the status quo? Do today’s algorithms reach human performance? For assessing the performance of state-of-the-art machine learning algorithms, we present a publicly available traffic sign dataset with more than 50,000 images of German road signs in 43 classes. The data was considered in the second stage of the German Traffic Sign Recognition Benchmark held at IJCNN 2011. The results of this competition are reported and the best-performing algorithms are briefly described. Convolutional neural networks (CNNs) showed particularly high classification accuracies in the competition. We measured the performance of human subjects on the same data—and the CNNs outperformed the human test persons."
}

@incollection{NIPS1989_250,
title = {Optimal Brain Damage},
author = {LeCun, Yann and John S. Denker and Sara A. Solla},
booktitle = {Advances in Neural Information Processing Systems 2},
editor = {D. S. Touretzky},
pages = {598--605},
year = 1990,
publisher = {Morgan-Kaufmann},
url = {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}
@ARTICLE{madry-iclr2018,
       author = {{Madry}, Aleksander and {Makelov}, Aleksandar and {Schmidt}, Ludwig and
         {Tsipras}, Dimitris and {Vladu}, Adrian},
        title = "{Towards Deep Learning Models Resistant to Adversarial Attacks}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
         year = "2017",
        month = "June",
          eid = {arXiv:1706.06083},
        pages = {arXiv:1706.06083},
archivePrefix = {arXiv},
       eprint = {1706.06083},
 primaryClass = {stat.ML},
        note={\url{https://ui.adsabs.harvard.edu/\#abs/2017arXiv170606083M}}
}
@misc{robustperturbationsoftware,
author = {Kevin Eykholt and others},
title = {{Public release of code for Robust Physical-World Attacks on Deep Learning Visual Classification (Eykholt et al., CVPR 2018)}},
timestamp = {Fri, 08 Jun 2018},
year = {2018},
howpublished={\url{https://github.com/evtimovi/robust_physical_perturbations}}
}

@InProceedings{roadsigns17,
                    author = {Kevin Eykholt and Ivan Evtimov and Earlence Fernandes and Bo Li and Amir Rahmati and Chaowei Xiao and Atul Prakash and Tadayoshi Kohno and Dawn Song},
                    title = {{Robust Physical-World Attacks on Deep Learning Visual Classification}},
                    booktitle = {Proc. Computer Vision and Pattern Recognition (CVPR). (See: \url{https://arxiv.org/abs/1707.08945})},
                    month = {June},
                    year = 2018,
		    pages = {1625--1634},
}

@ARTICLE{2014arXiv1412.6572G,
       author = {{Goodfellow}, Ian J. and {Shlens}, Jonathon and {Szegedy}, Christian},
        title = "{Explaining and Harnessing Adversarial Examples}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2014",
        month = "Dec",
          eid = {arXiv:1412.6572},
        pages = {arXiv:1412.6572},
archivePrefix = {arXiv},
       eprint = {1412.6572},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6572G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{liu2017trojaning,
  title={Trojaning attack on neural networks},
  author={Liu, Yingqi and Ma, Shiqing and Aafer, Yousra and Lee, Wen-Chuan and Zhai, Juan and Wang, Weihang and Zhang, Xiangyu},
  year={2017}
}
@ARTICLE{2017arXiv170806733G,
       author = {{Gu}, Tianyu and {Dolan-Gavitt}, Brendan and {Garg}, Siddharth},
        title = "{BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = "2017",
        month = "Aug",
          eid = {arXiv:1708.06733},
        pages = {arXiv:1708.06733},
archivePrefix = {arXiv},
       eprint = {1708.06733},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170806733G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv180512185L,
       author = {{Liu}, Kang and {Dolan-Gavitt}, Brendan and {Garg}, Siddharth},
        title = "{Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
         year = "2018",
        month = "May",
          eid = {arXiv:1805.12185},
        pages = {arXiv:1805.12185},
archivePrefix = {arXiv},
       eprint = {1805.12185},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180512185L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

